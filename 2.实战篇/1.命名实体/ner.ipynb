{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# 基于Transformers的命名实体识别",
   "id": "87b6efd72132a8cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.导包",
   "id": "6ef3165e28f85d60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:03:33.576460Z",
     "start_time": "2025-11-22T03:03:24.944488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer,AutoModelForTokenClassification,TrainingArguments,Trainer,DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader"
   ],
   "id": "75e3b18ffc94d604",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.加载数据集",
   "id": "4a4cfe7be922cee5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:03:38.406332Z",
     "start_time": "2025-11-22T03:03:33.609356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ner_dataset = load_dataset(\"lansinuote/peoples-daily-ner\")\n",
    "ner_dataset"
   ],
   "id": "486318891c5bf1c8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 20865\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 2319\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 4637\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:03:38.507462Z",
     "start_time": "2025-11-22T03:03:38.502462Z"
    }
   },
   "cell_type": "code",
   "source": "ner_dataset[\"train\"]",
   "id": "294baa0dcc07fbe2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags'],\n",
       "    num_rows: 20865\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:03:38.609730Z",
     "start_time": "2025-11-22T03:03:38.605216Z"
    }
   },
   "cell_type": "code",
   "source": "ner_dataset[\"train\"][0]",
   "id": "88a0b3a3ce5ef12e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['海',\n",
       "  '钓',\n",
       "  '比',\n",
       "  '赛',\n",
       "  '地',\n",
       "  '点',\n",
       "  '在',\n",
       "  '厦',\n",
       "  '门',\n",
       "  '与',\n",
       "  '金',\n",
       "  '门',\n",
       "  '之',\n",
       "  '间',\n",
       "  '的',\n",
       "  '海',\n",
       "  '域',\n",
       "  '。'],\n",
       " 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:03:38.683050Z",
     "start_time": "2025-11-22T03:03:38.677385Z"
    }
   },
   "cell_type": "code",
   "source": "ner_dataset[\"train\"].features",
   "id": "697e7e15e6767142",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value('string'),\n",
       " 'tokens': List(Value('string')),\n",
       " 'ner_tags': List(ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']))}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:03:38.954556Z",
     "start_time": "2025-11-22T03:03:38.949038Z"
    }
   },
   "cell_type": "code",
   "source": "ner_dataset[\"train\"].features[\"ner_tags\"]",
   "id": "663442e89d86cd3d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List(ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:03:39.346797Z",
     "start_time": "2025-11-22T03:03:39.339786Z"
    }
   },
   "cell_type": "code",
   "source": "ner_dataset[\"train\"].features[\"ner_tags\"].feature",
   "id": "9ecaa3bbb450b599",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:03:39.477600Z",
     "start_time": "2025-11-22T03:03:39.471489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_list = ner_dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_list # 最后会有一步文本映射"
   ],
   "id": "369afb17f6a29ce4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.数据集预处理",
   "id": "2ce73f5387689ac6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:03:40.527746Z",
     "start_time": "2025-11-22T03:03:39.618463Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")",
   "id": "58e68a1554b8850",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:03:40.553085Z",
     "start_time": "2025-11-22T03:03:40.545252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "to = tokenizer(ner_dataset[\"train\"][0][\"tokens\"], is_split_into_words=True) # 将拆分的词组合回一句话\n",
    "to # 测试用，不用于主代码"
   ],
   "id": "52dfda6bc9a3cdc4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 3862, 7157, 3683, 6612, 1765, 4157, 1762, 1336, 7305, 680, 7032, 7305, 722, 7313, 4638, 3862, 1818, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:03:40.580564Z",
     "start_time": "2025-11-22T03:03:40.575609Z"
    }
   },
   "cell_type": "code",
   "source": "to.word_ids()",
   "id": "92891f8d8508e16e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:03:40.620282Z",
     "start_time": "2025-11-22T03:03:40.614650Z"
    }
   },
   "cell_type": "code",
   "source": "ner_dataset[\"train\"][\"ner_tags\"][0]",
   "id": "d0000c503e5ba4d9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:03:40.645529Z",
     "start_time": "2025-11-22T03:03:40.641288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_function(examples):\n",
    "    tokenizer_examples = tokenizer(examples[\"tokens\"], max_length=128, truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]): # examples[\"ner_tags\"] = ner_dataset[\"train和validation和test\"][\"ner_tags\"],\n",
    "        word_ids = tokenizer_examples.word_ids(batch_index=i) # 相当于前文的to.word_ids()\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100) # -100表示忽略\n",
    "            else:\n",
    "                label_ids.append(label[word_id]) # 去除None\n",
    "        labels.append(label_ids)\n",
    "    tokenizer_examples[\"labels\"] = labels\n",
    "    return tokenizer_examples"
   ],
   "id": "6725c07c573d438a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:03:41.887057Z",
     "start_time": "2025-11-22T03:03:40.664538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer_dataset = ner_dataset.map(process_function, batched=True)\n",
    "tokenizer_dataset"
   ],
   "id": "f06b67c945f705e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4637 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6b4d2e2c81d4bb7809c20833b00dae0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20865\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2319\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4637\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:03:41.920832Z",
     "start_time": "2025-11-22T03:03:41.916732Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer_dataset[\"train\"][0])",
   "id": "877d91d3a2b4f63a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['海', '钓', '比', '赛', '地', '点', '在', '厦', '门', '与', '金', '门', '之', '间', '的', '海', '域', '。'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 3862, 7157, 3683, 6612, 1765, 4157, 1762, 1336, 7305, 680, 7032, 7305, 722, 7313, 4638, 3862, 1818, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0, -100]}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.创建模型",
   "id": "facce2ec6abd05df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:11:47.610638Z",
     "start_time": "2025-11-22T03:11:46.059216Z"
    }
   },
   "cell_type": "code",
   "source": "model = AutoModelForTokenClassification.from_pretrained(\"hfl/chinese-macbert-base\", num_labels=len(label_list))",
   "id": "c998745c1c2719c3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T07:49:44.920819Z",
     "start_time": "2025-11-22T07:49:44.913680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.config.id2label = {idx: label for idx, label in enumerate(label_list)}\n",
    "model.config"
   ],
   "id": "2df51b98a55fe427",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForTokenClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"dtype\": \"float32\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"O\",\n",
       "    \"1\": \"B-PER\",\n",
       "    \"2\": \"I-PER\",\n",
       "    \"3\": \"B-ORG\",\n",
       "    \"4\": \"I-ORG\",\n",
       "    \"5\": \"B-LOC\",\n",
       "    \"6\": \"I-LOC\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.57.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    ## 5.创建评估函数",
   "id": "f5bdcfd1fc40bbbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:12:57.755808Z",
     "start_time": "2025-11-22T03:12:53.602323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "seqeval"
   ],
   "id": "cc61fffc8b857ef0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"seqeval\", module_type: \"metric\", features: {'predictions': List(Value('string')), 'references': List(Value('string'))}, usage: \"\"\"\n",
       "Produces labelling scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions: List of List of predicted labels (Estimated targets as returned by a tagger)\n",
       "    references: List of List of reference labels (Ground truth (correct) target values)\n",
       "    suffix: True if the IOB prefix is after type, False otherwise. default: False\n",
       "    scheme: Specify target tagging scheme. Should be one of [\"IOB1\", \"IOB2\", \"IOE1\", \"IOE2\", \"IOBES\", \"BILOU\"].\n",
       "        default: None\n",
       "    mode: Whether to count correct entity labels with incorrect I/B tags as true positives or not.\n",
       "        If you want to only count exact matches, pass mode=\"strict\". default: None.\n",
       "    sample_weight: Array-like of shape (n_samples,), weights for individual samples. default: None\n",
       "    zero_division: Which value to substitute as a metric value when encountering zero division. Should be on of 0, 1,\n",
       "        \"warn\". \"warn\" acts as 0, but the warning is raised.\n",
       "\n",
       "Returns:\n",
       "    'scores': dict. Summary of the scores for overall and per type\n",
       "        Overall:\n",
       "            'accuracy': accuracy,\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': F1 score, also known as balanced F-score or F-measure,\n",
       "        Per type:\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': F1 score, also known as balanced F-score or F-measure\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
       "    >>> references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
       "    >>> seqeval = evaluate.load(\"seqeval\")\n",
       "    >>> results = seqeval.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['MISC', 'PER', 'overall_precision', 'overall_recall', 'overall_f1', 'overall_accuracy']\n",
       "    >>> print(results[\"overall_f1\"])\n",
       "    0.5\n",
       "    >>> print(results[\"PER\"][\"f1\"])\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:14:48.522253Z",
     "start_time": "2025-11-22T03:14:48.516237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "def eval_metric(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    true_predictions = [[label_list[p] for p, l in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    true_labels = [[label_list[l] for p, l in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels, mode=\"strict\", scheme=\"IOB2\")\n",
    "    return {\n",
    "        \"f1\": results[\"overall_f1\"]\n",
    "    }\n"
   ],
   "id": "28f5b377e31054f2",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6.配置训练参数",
   "id": "8bd032128f2299fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:15:44.505337Z",
     "start_time": "2025-11-22T03:15:44.455321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"models_for_ner\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=128,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3\n",
    ")"
   ],
   "id": "f0279682e3ad49fc",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.创建训练器",
   "id": "18a65d799aab85e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T03:15:45.944409Z",
     "start_time": "2025-11-22T03:15:45.931934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenizer_dataset[\"train\"],\n",
    "    eval_dataset=tokenizer_dataset[\"validation\"],\n",
    "    compute_metrics=eval_metric,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    ")"
   ],
   "id": "ae4c56e010f4943",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8.模型训练",
   "id": "152587fc27aca806"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T07:17:43.449644Z",
     "start_time": "2025-11-22T03:15:46.901545Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "dc23693ded0576b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='981' max='981' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [981/981 4:01:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.021337</td>\n",
       "      <td>0.945560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>0.951303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.017926</td>\n",
       "      <td>0.957689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=981, training_loss=0.02451849159965943, metrics={'train_runtime': 14516.0382, 'train_samples_per_second': 4.312, 'train_steps_per_second': 0.068, 'total_flos': 3940951205762142.0, 'train_loss': 0.02451849159965943, 'epoch': 3.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T07:20:41.595529Z",
     "start_time": "2025-11-22T07:17:43.571182Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.evaluate(eval_dataset=tokenizer_dataset[\"test\"])",
   "id": "69d85a2f18e2e067",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37/37 02:51]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.023530589416623116,\n",
       " 'eval_f1': 0.9467487289269467,\n",
       " 'eval_runtime': 178.0055,\n",
       " 'eval_samples_per_second': 26.05,\n",
       " 'eval_steps_per_second': 0.208,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9.模型预测",
   "id": "cb1973788fd32bf0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T07:20:41.649653Z",
     "start_time": "2025-11-22T07:20:41.646613Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import pipeline",
   "id": "372e442bb7ed5b4b",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T07:55:40.892148Z",
     "start_time": "2025-11-22T07:55:40.826688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ner_pipe = pipeline(\"token-classification\",model=model,tokenizer=tokenizer,device=0,aggregation_strategy=\"simple\")\n",
    "res = ner_pipe(\"我叫张三，我今年18岁，我住在上海。\")\n",
    "res"
   ],
   "id": "6d2ffb360ba4971d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9994254),\n",
       "  'word': '张 三',\n",
       "  'start': 2,\n",
       "  'end': 4},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9994155),\n",
       "  'word': '上 海',\n",
       "  'start': 15,\n",
       "  'end': 17}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T07:58:07.027396Z",
     "start_time": "2025-11-22T07:58:07.022241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ner_result = {}\n",
    "x = \"我叫张三，我今年18岁，我住在上海。\"\n",
    "for r in res:\n",
    "    if r['entity_group'] not in ner_result:\n",
    "        ner_result[r['entity_group']] = []\n",
    "    ner_result[r['entity_group']].append(x[r['start']:r['end']])\n",
    "ner_result"
   ],
   "id": "25173b3ada1bb073",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PER': ['张三'], 'LOC': ['上海']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1eb4e97c1ad7dd69"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
